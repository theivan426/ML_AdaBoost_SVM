{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1d2d16",
   "metadata": {},
   "source": [
    "## 1. Image Classification with Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed4bf0",
   "metadata": {},
   "source": [
    "## Image pre-proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ca2ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten each image, add sub-sliding windows features to each image, \n",
    "\n",
    "def obtain_dataset(folder_name):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import glob\n",
    "    from matplotlib import pyplot as plt\n",
    "    # assuming 128x128 size images and HoGDescriptor length of 34020\n",
    "    X, X_1, y = [], [], []\n",
    "\n",
    "    for fullpath in glob.iglob(f'{folder_name}/*/*'):\n",
    "        _, target, _ = fullpath.split('/')\n",
    "        X_1.append(cv2.imread(fullpath))\n",
    "        X.append(cv2.imread(fullpath).flatten())\n",
    "        y.append(target)\n",
    "\n",
    "    X, X_1, y = np.array(X), np.array(X_1), np.array(y)   \n",
    "  #  print(X.shape, X_1.shape)\n",
    "    hog_feature_len=34020\n",
    "    hog = cv2.HOGDescriptor()\n",
    "  #  for image in X_1:\n",
    "  #  h = hog.compute(image)\n",
    "\n",
    "    W = []\n",
    "    stepSize = 30\n",
    "    (w_width, w_height) = (32, 32) # window size\n",
    "    for image in X_1:\n",
    "        for i in range(0, 128 - w_width , stepSize):\n",
    "               for j in range(0, 128 - w_height, stepSize):\n",
    "                    window = image[i:i + w_width, j:j + w_height,:]\n",
    "        W.append(window.flatten())\n",
    "\n",
    "    W = np.array(W)\n",
    "  #  print(W.shape)\n",
    "    X = np.hstack((W,X))\n",
    "\n",
    "    y    = (y[:,None] == np.unique(y)[None]).argmax(axis = 1)\n",
    "    # use this to read all images in the three directories and obtain the set of features X and train labels Y\n",
    "    # you can assume there are three different classes in the image dataset\n",
    "    return (X,y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b97e9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional function for those who want to include pre-processing for train data in obtain dataset\n",
    "def obtain_dataset_train_test(folder_name_train, folder_name_test):\n",
    " \n",
    "    \n",
    "    X_train, y_train = obtain_dataset(folder_name_train)\n",
    "    X_test,  y_test  = obtain_dataset(folder_name_test)\n",
    "\n",
    "    return (X_train, y_train, X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811636e5",
   "metadata": {},
   "source": [
    "## AdaBoost Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b090477b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BoostingClassifier:\n",
    "    import numpy as np\n",
    "    '''\n",
    "    Parameters\n",
    "    -----------\n",
    "    base_estimator: object\n",
    "        The base model from which the boosted ensemble is built.\n",
    "\n",
    "    n_estimators: integer, optional(default=50)\n",
    "        The maximum number of estimators\n",
    "\n",
    "    learning_rate: float, optional(default=1)\n",
    "\n",
    "    algorithm: {'SAMME','SAMME.R'}, optional(default='SAMME.R')\n",
    "        SAMME.R uses predicted probabilities to update wights, while SAMME uses class error rate\n",
    "\n",
    "    random_state: int or None, optional(default=None)\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        import numpy as np\n",
    "        from copy import deepcopy\n",
    "        if kwargs and args:\n",
    "            raise ValueError(\n",
    "                '''AdaBoostClassifier can only be called with keyword\n",
    "                   arguments for the following keywords: base_estimator ,n_estimators,\n",
    "                    learning_rate,algorithm,random_state''')\n",
    "        allowed_keys = ['base_estimator', 'n_estimators', 'learning_rate', 'algorithm', 'random_state']\n",
    "        keywords_used = kwargs.keys()\n",
    "        for keyword in keywords_used:\n",
    "            if keyword not in allowed_keys:\n",
    "                raise ValueError(keyword + \":  Wrong keyword used --- check spelling\")\n",
    "\n",
    "        n_estimators = 50\n",
    "        learning_rate = 1\n",
    "        algorithm = 'SAMME.R'\n",
    "        random_state = None\n",
    "\n",
    "        if kwargs and not args:\n",
    "            if 'base_estimator' in kwargs:\n",
    "                base_estimator = kwargs.pop('base_estimator')\n",
    "            else:\n",
    "                raise ValueError('''base_estimator can not be None''')\n",
    "            if 'n_estimators' in kwargs: n_estimators = kwargs.pop('n_estimators')\n",
    "            if 'learning_rate' in kwargs: learning_rate = kwargs.pop('learning_rate')\n",
    "            if 'algorithm' in kwargs: algorithm = kwargs.pop('algorithm')\n",
    "            if 'random_state' in kwargs: random_state = kwargs.pop('random_state')\n",
    "\n",
    "        self.base_estimator_ = base_estimator\n",
    "        self.n_estimators_ = n_estimators\n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.algorithm_ = algorithm\n",
    "        self.random_state_ = random_state\n",
    "        self.estimators_ = list()\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators_)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators_)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        import numpy as np\n",
    "        from copy import deepcopy\n",
    "        return {\"base_estimator\": self.base_estimator,\"n_estimators\": self.n_estimators, \"learning_rate\": self.learning_rate,\\\n",
    "                \"algorithm\": self.algorithm, \"random_state\": self.random_state}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        import numpy as np\n",
    "        from copy import deepcopy\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def _samme_proba(self, estimator, n_classes, X):\n",
    "        import numpy as np\n",
    "        from copy import deepcopy\n",
    "\n",
    "        proba = estimator.predict_proba(X)\n",
    "\n",
    "        # Displace zero probabilities so the log is defined.\n",
    "        # Also fix negative elements which may occur with\n",
    "        # negative sample weights.\n",
    "        proba[proba < np.finfo(proba.dtype).eps] = np.finfo(proba.dtype).eps\n",
    "        log_proba = np.log(proba)\n",
    "\n",
    "        return (n_classes - 1) * (log_proba - (1. / n_classes)\n",
    "                                  * log_proba.sum(axis=1)[:, np.newaxis])\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        import numpy as np\n",
    "        from copy import deepcopy\n",
    "        self.n_samples = X.shape[0]\n",
    "        # There is hidden trouble for classes, here the classes will be sorted.\n",
    "        # So in boost we have to ensure that the predict results have the same classes sort\n",
    "        self.classes_ = np.array(sorted(list(set(y))))\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        for iboost in range(self.n_estimators_):\n",
    "            if iboost == 0:\n",
    "                sample_weight = np.ones(self.n_samples) / self.n_samples\n",
    "\n",
    "            sample_weight, estimator_weight, estimator_error = self.boost(X, y, sample_weight)\n",
    "\n",
    "            # early stop\n",
    "            if estimator_error == None:\n",
    "                break\n",
    "\n",
    "            # append error and weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "\n",
    "            if estimator_error <= 0:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def boost(self, X, y, sample_weight):\n",
    "        import numpy as np\n",
    "        from copy import deepcopy\n",
    "        if self.algorithm_ == 'SAMME':\n",
    "            return self.discrete_boost(X, y, sample_weight)\n",
    "        elif self.algorithm_ == 'SAMME.R':\n",
    "            return self.real_boost(X, y, sample_weight)\n",
    "\n",
    "    def real_boost(self, X, y, sample_weight):\n",
    "        import numpy as np\n",
    "        from copy import deepcopy\n",
    "        from numpy.core.umath_tests import inner1d\n",
    "        estimator = deepcopy(self.base_estimator_)\n",
    "        if self.random_state_:\n",
    "            estimator.set_params(random_state=1)\n",
    "\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        y_pred = estimator.predict(X)\n",
    "        incorrect = y_pred != y\n",
    "        estimator_error = np.dot(incorrect, sample_weight) / np.sum(sample_weight, axis=0)\n",
    "\n",
    "        # if worse than random guess, stop boosting\n",
    "        if estimator_error >= 1.0 - 1 / self.n_classes_:\n",
    "            return None, None, None\n",
    "\n",
    "        y_predict_proba = estimator.predict_proba(X)\n",
    "        # repalce zero\n",
    "        y_predict_proba[y_predict_proba < np.finfo(y_predict_proba.dtype).eps] = np.finfo(y_predict_proba.dtype).eps\n",
    "\n",
    "        y_codes = np.array([-1. / (self.n_classes_ - 1), 1.])\n",
    "        y_coding = y_codes.take(self.classes_ == y[:, np.newaxis])\n",
    "\n",
    "        # for sample weight update\n",
    "        intermediate_variable = (-1. * self.learning_rate_ * (((self.n_classes_ - 1) / self.n_classes_) *\n",
    "                                                              inner1d(y_coding, np.log(\n",
    "                                                                  y_predict_proba))))  #dot iterate for each row\n",
    "\n",
    "        # update sample weight\n",
    "        sample_weight *= np.exp(intermediate_variable)\n",
    "\n",
    "        sample_weight_sum = np.sum(sample_weight, axis=0)\n",
    "        if sample_weight_sum <= 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # normalize sample weight\n",
    "        sample_weight /= sample_weight_sum\n",
    "\n",
    "        # append the estimator\n",
    "        self.estimators_.append(estimator)\n",
    "\n",
    "        return sample_weight, 1, estimator_error\n",
    "\n",
    "\n",
    "    def discrete_boost(self, X, y, sample_weight):\n",
    "        import numpy as np\n",
    "        from copy import deepcopy\n",
    "        estimator = deepcopy(self.base_estimator_)\n",
    "        if self.random_state_:\n",
    "            estimator.set_params(random_state=1)\n",
    "\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        y_pred = estimator.predict(X)\n",
    "        incorrect = y_pred != y\n",
    "        estimator_error = np.dot(incorrect, sample_weight) / np.sum(sample_weight, axis=0)\n",
    "\n",
    "        # if worse than random guess, stop boosting\n",
    "        if estimator_error >= 1 - 1 / self.n_classes_:\n",
    "            return None, None, None\n",
    "\n",
    "        # update estimator_weight\n",
    "        estimator_weight = self.learning_rate_ * np.log((1 - estimator_error) / estimator_error) + np.log(\n",
    "            self.n_classes_ - 1)\n",
    "\n",
    "        if estimator_weight <= 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # update sample weight\n",
    "        sample_weight *= np.exp(estimator_weight * incorrect)\n",
    "\n",
    "        sample_weight_sum = np.sum(sample_weight, axis=0)\n",
    "        if sample_weight_sum <= 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # normalize sample weight\n",
    "        sample_weight /= sample_weight_sum\n",
    "\n",
    "        # append the estimator\n",
    "        self.estimators_.append(estimator)\n",
    "\n",
    "        return sample_weight, estimator_weight, estimator_error\n",
    "\n",
    "    def predict(self, X):\n",
    "        import numpy as np\n",
    "        from copy import deepcopy\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "        pred = None\n",
    "\n",
    "        if self.algorithm_ == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            pred = sum(self._samme_proba(estimator, n_classes, X) for estimator in self.estimators_)\n",
    "        else:  # self.algorithm == \"SAMME\"\n",
    "            pred = sum((estimator.predict(X) == classes).T * w\n",
    "                       for estimator, w in zip(self.estimators_,\n",
    "                                               self.estimator_weights_))\n",
    "\n",
    "        pred /= self.estimator_weights_.sum()\n",
    "        if n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            pred = pred.sum(axis=1)\n",
    "            return self.classes_.take(pred > 0, axis=0)\n",
    "\n",
    "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        import numpy as np\n",
    "        from copy import deepcopy\n",
    "        if self.algorithm_ == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            proba = sum(self._samme_proba(estimator, self.n_classes_, X)\n",
    "                        for estimator in self.estimators_)\n",
    "        else:  # self.algorithm == \"SAMME\"\n",
    "            proba = sum(estimator.predict_proba(X) * w\n",
    "                        for estimator, w in zip(self.estimators_,\n",
    "                                                self.estimator_weights_))\n",
    "\n",
    "        proba /= self.estimator_weights_.sum()\n",
    "        proba = np.exp((1. / (n_classes - 1)) * proba)\n",
    "        normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "        normalizer[normalizer == 0.0] = 1.0\n",
    "        proba /= normalizer\n",
    "\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f311be",
   "metadata": {},
   "source": [
    "### Adaboost Accuracy\n",
    "#### Using optimised hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58533d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_boosting_image(image_dataset_train, image_dataset_test):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    #(X_train, Y_train) = obtain_dataset(image_dataset_train)\n",
    "    #(X_test, Y_test) = obtain_dataset(image_dataset_test)# optionally replace the two calls with a single call to obtain_dataset_train_test() function\n",
    "    (X_train, Y_train, X_test, Y_test) = obtain_dataset_train_test(image_dataset_train, image_dataset_test)\n",
    "    bc = BoostingClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=0.25,algorithm='SAMME')\n",
    "    bc.fit(X_train, Y_train)\n",
    "    y_pred = bc.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6437d45b",
   "metadata": {},
   "source": [
    "## 2. Image Classification with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa524e21",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c99ba5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier: # NOTE sklearn Classifier is used instead so left this cell blank\n",
    "    def __init__(self):\n",
    "        #implement initialisation\n",
    "        self.some_paramter=1\n",
    "    def fit_image(self, X,y):\n",
    "        #training of the SVM \n",
    "        # providing for separate image kernels\n",
    "        return \n",
    "    def fit_text(self, X,y):\n",
    "        # training of the SVM\n",
    "        # providing for separate text kernels\n",
    "        return\n",
    "    def predict_image(self, X):\n",
    "        # prediction routine for the SVM\n",
    "        return\n",
    "    def predict_text(self, X):\n",
    "        # prediction routine for the SVM\n",
    "        return    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9175d9cf",
   "metadata": {},
   "source": [
    "### Custom Kernals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d3a0e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - Custom: Cauchy...\n",
    "\n",
    "def cauchy(i, j, sigma=None):\n",
    "    '''\n",
    "        K(x, y) = 1 / (1 + ||x - y||^2 / s ^ 2)\n",
    "    where:\n",
    "        s = sigma\n",
    "    '''\n",
    "    from scipy.spatial import distance\n",
    "    \n",
    "    if sigma is None:\n",
    "        sigma = float(i.shape[1])\n",
    "        dists_sq = distance.cdist(i, j, 'euclidean')\n",
    "\n",
    "    return 1 / (1 + dists_sq / sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf2f3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - Custom: Cosine kernal...\n",
    "\n",
    "def cosine(i, j):\n",
    "    import numpy as np\n",
    "    \"\"\" \n",
    "        K(x, y) = <x . y> / (||x|| . ||y||)\n",
    "    \"\"\"\n",
    "    norm_i = np.linalg.norm(i)\n",
    "    norm_j = np.linalg.norm(j)\n",
    "    return i.dot(j.T) / (norm_i * (norm_j.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c4cf841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Custom: Multi-quadric kernal...\n",
    "\n",
    "def multiquadric(i, j):\n",
    "    import numpy as np\n",
    "    from scipy.spatial import distance\n",
    "    \"\"\"\n",
    "    Multiquadratic kernel, \n",
    "        K(x, y) = sqrt(||x-y||^2 + c^2)\n",
    "    where:\n",
    "        c > 0\n",
    "    \"\"\"\n",
    "    c = 0.5\n",
    "    dists_sq = distance.cdist(i, j, 'euclidean')\n",
    "    return np.sqrt(dists_sq + c**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c1870e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 - Custom: Inverse Multi-quadric kernal...\n",
    "\n",
    "def inv_multiquadric(i, j):\n",
    "    from scipy.spatial import distance\n",
    "    import numpy as np\n",
    "    \"\"\"\n",
    "    Inverse Multiquadratic kernel, \n",
    "        K(x, y) = 1/ sqrt(||x-y||^2 + c^2)\n",
    "    where:\n",
    "        c > 0\n",
    "    \"\"\"\n",
    "    c = 0.5\n",
    "    dists_sq = distance.cdist(i, j, 'euclidean')\n",
    "    return 1 / np.sqrt(dists_sq + c**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3d75f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 - Custom: T-Student kernal...\n",
    "def t_student(d=50):\n",
    "    def t_student_kernel(i, j):\n",
    "        from scipy.spatial import distance\n",
    "        import numpy as np\n",
    "        \"\"\"\n",
    "        T-Student kernel, \n",
    "            K(x, y) = 1 / (1 + ||x - y||^d)\n",
    "        where:\n",
    "            d = degree\n",
    "        \"\"\"\n",
    "        sqrt_dist = np.sqrt((distance.cdist(i, j, 'euclidean')))\n",
    "        return 1 / (1 + sqrt_dist ** d)\n",
    "    return t_student_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac84943",
   "metadata": {},
   "source": [
    "### SVM Accuracy \n",
    "#### Using optimised hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da669c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_svm_image(image_dataset_train, image_dataset_test):\n",
    "    from sklearn.metrics import accuracy_score    \n",
    "    from sklearn.svm import SVC\n",
    "    #(X_train, Y_train) = obtain_dataset(image_dataset_train)\n",
    "    #(X_test, Y_test) = obtain_dataset(image_dataset_test) # optionally replace the two calls with a single call to obtain_dataset_train_test() function\n",
    "    (X_train, Y_train, X_test, Y_test) = obtain_dataset_train_test(image_dataset_train, image_dataset_test)\n",
    "    sc = SVC(kernel = 'poly',decision_function_shape='ovr', C = 100, gamma = 1)\n",
    "    sc.fit(X_train, Y_train)\n",
    "    y_pred = sc.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d71b43",
   "metadata": {},
   "source": [
    "## 3. Text Classification/ Sentiment Analysis with Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c241c7",
   "metadata": {},
   "source": [
    "## Text data pre-proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20ddd4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bag_of_words(train_file):\n",
    "    # Write your preprocessor to process the text\n",
    "    # Write your own bag of words feature extractor using nltk and scikit-learn\n",
    "    # return (X,y)\n",
    "    import nltk\n",
    "    #nltk.download()\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df = pd.read_csv(train_file, index_col=False)\n",
    " #   print(len(df))\n",
    " #   print(df.sentiment.value_counts())\n",
    "    # Cleaning data...\n",
    "\n",
    "    # Make input all lower case...\n",
    "    df['review'] = df['review'].str.lower()\n",
    "\n",
    "    # Remove punctuations...\n",
    "    df['review'] = df['review'].str.replace(r'[^\\w\\s]+', '')\n",
    "\n",
    "    # Tokenise words...\n",
    "    df['review'] = df['review'].apply(word_tokenize)\n",
    "\n",
    "    # Remove stop-words...\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def remove_stops(row):\n",
    "        my_list = row['review']\n",
    "        meaningful_words = [w for w in my_list if not w in stop_words]\n",
    "        return (meaningful_words)\n",
    "\n",
    "    df['review_clean'] = df.apply(remove_stops, axis=1)\n",
    "\n",
    "    # check an example\n",
    "    #print(df['review_clean'].iloc[1])\n",
    "    \n",
    "    # implement stemming...\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    def stem_list(row):\n",
    "        my_list = row['review_clean']\n",
    "        stemmed_list = [ps.stem(word) for word in my_list]\n",
    "        return (stemmed_list)\n",
    "\n",
    "    df['review_stemmed'] = df.apply(stem_list, axis=1)\n",
    "    \n",
    "    X = df.review_stemmed\n",
    "    y = np.array(df.sentiment)\n",
    "\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "842a642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bag_of_words_train_test(train_file, test_file):\n",
    "    # Write your preprocessor to process the text\n",
    "    # Write your own bag of words feature extractor using nltk and scikit-learn\n",
    "    \n",
    "    # Process training data first and ensure the test data is not used while extracting bag of words feature vector\n",
    "    # 80%/ 20% train test data set...\n",
    "    # x_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    \n",
    "    x_train, y_train = extract_bag_of_words(train_file)     # X = df.review_stemmed; y = np.array(df.sentiment)\n",
    "    X_test, y_test = extract_bag_of_words(test_file)\n",
    " #   X_test = np.array(X_test)\n",
    "    \n",
    "    print(\"Train shapes : X = {}, y = {}\".format(x_train.shape,y_train.shape))\n",
    "    print(\"Test shapes : X = {}, y = {}\".format(X_test.shape,y_test.shape))\n",
    "    \n",
    "    # Process testing data here. Ensure that test data is not used above\n",
    "    \n",
    "    # Implement bag of words TfidfVectorizer...\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=20000)\n",
    "\n",
    "    # fit and tranform training and test set text into a matrix separately \n",
    "    X_train_tfidf = tfidf.fit_transform(x_train.map(' '.join)).toarray()\n",
    "    X_test_tfidf = tfidf.transform(X_test.map(' '.join)).toarray() # took out fit\n",
    "\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    \n",
    "    \n",
    "    svd = TruncatedSVD(n_components=500, random_state=42)\n",
    "   \n",
    "    X_train = svd.fit_transform(X_train_tfidf)\n",
    "    X_test = svd.transform(X_test_tfidf)\n",
    "    \n",
    "    print('LSA output shape:', X_train.shape)\n",
    "    \n",
    "    #for col in X_train.nonzero()[1]:\n",
    "    #    print(feature_names[col], ' - ', X_train[0, col])\n",
    "    \n",
    " #   print(\"num_features: {}\".format(len(feature_names)))\n",
    "    print(\"X_train.shape = \",X_train.shape)\n",
    "    print(\"X_test.shape = \",X_test.shape)\n",
    "    print(\"y_train.shape = \",y_train.shape)\n",
    "    print(\"y_test.shape = \",y_test.shape)\n",
    " #   print('X_train vectorised output sample:\\n {}'.format(X_train[0:15]))\n",
    "    # return (X_train,y_train,X_test,y_test)\n",
    "\n",
    "    return (X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607b34b9",
   "metadata": {},
   "source": [
    "## Adaboost Accuracy\n",
    "#### Using optimised hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a484de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Text Classification/ Sentiment Analysis with Adaboostdef test_func_boosting_text(text_dataset_train, text_dataset_test):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "#    (X_train, Y_train) = extract_bag_of_words(text_dataset_train)\n",
    "#    (X_test, Y_test) = extract_bag_of_words(text_dataset_test) # optionally the two calls can be replaced by a single extract_bag_of_words_train_test() function\n",
    "    (X_train,Y_train,X_test,Y_test) = extract_bag_of_words_train_test(text_dataset_train, text_dataset_test)    \n",
    "    bc = BoostingClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=150, learning_rate=1,algorithm='SAMME')\n",
    "    bc.fit(X_train, Y_train)\n",
    "    y_pred = bc.predict(X_test)    \n",
    "    acc = accuracy_score(Y_test, y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d1aea6",
   "metadata": {},
   "source": [
    "## 4. Text Classification/ Sentiment Analysis with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9aac7f",
   "metadata": {},
   "source": [
    "### Custom Kernals for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe4faf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - Custom: Cauchy...\n",
    "\n",
    "def cauchy(i, j, sigma=None):\n",
    "    '''\n",
    "        K(x, y) = 1 / (1 + ||x - y||^2 / s ^ 2)\n",
    "    where:\n",
    "        s = sigma\n",
    "    '''\n",
    "    from scipy.spatial import distance\n",
    "    \n",
    "    if sigma is None:\n",
    "        sigma = float(i.shape[1])\n",
    "        dists_sq = distance.cdist(i, j, 'euclidean')\n",
    "\n",
    "    return 1 / (1 + dists_sq / sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21422eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - Custom: Cosine kernal...\n",
    "\n",
    "def cosine(i, j):\n",
    "    import numpy as np\n",
    "    \"\"\" \n",
    "        K(x, y) = <x . y> / (||x|| . ||y||)\n",
    "    \"\"\"\n",
    "    norm_i = np.linalg.norm(i)\n",
    "    norm_j = np.linalg.norm(j)\n",
    "    return i.dot(j.T) / (norm_i * (norm_j.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e365b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Custom: Multi-quadric kernal...\n",
    "\n",
    "def multiquadric(i, j):\n",
    "    import numpy as np\n",
    "    from scipy.spatial import distance\n",
    "    \"\"\"\n",
    "    Multiquadratic kernel, \n",
    "        K(x, y) = sqrt(||x-y||^2 + c^2)\n",
    "    where:\n",
    "        c > 0\n",
    "    \"\"\"\n",
    "    c = 0.5\n",
    "    dists_sq = distance.cdist(i, j, 'euclidean')\n",
    "    return np.sqrt(dists_sq + c**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51b97f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 - Custom: Inverse Multi-quadric kernal...\n",
    "\n",
    "def inv_multiquadric(i, j):\n",
    "    from scipy.spatial import distance\n",
    "    import numpy as np\n",
    "    \"\"\"\n",
    "    Inverse Multiquadratic kernel, \n",
    "        K(x, y) = 1/ sqrt(||x-y||^2 + c^2)\n",
    "    where:\n",
    "        c > 0\n",
    "    \"\"\"\n",
    "    c = 0.5\n",
    "    dists_sq = distance.cdist(i, j, 'euclidean')\n",
    "    return 1 / np.sqrt(dists_sq + c**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7af688aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 - Custom: T-Student kernal...\n",
    "def t_student(d=50):\n",
    "    def t_student_kernel(i, j):\n",
    "        from scipy.spatial import distance\n",
    "        import numpy as np\n",
    "        \"\"\"\n",
    "        T-Student kernel, \n",
    "            K(x, y) = 1 / (1 + ||x - y||^d)\n",
    "        where:\n",
    "            d = degree\n",
    "        \"\"\"\n",
    "        sqrt_dist = np.sqrt((distance.cdist(i, j, 'euclidean')))\n",
    "        return 1 / (1 + sqrt_dist ** d)\n",
    "    return t_student_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0c74e",
   "metadata": {},
   "source": [
    "## SVM Accuracy\n",
    "\n",
    "#### Using optimised hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5d79eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_svm_text(text_dataset_train, text_dataset_test):\n",
    "    from sklearn.metrics import accuracy_score    \n",
    "    from sklearn.svm import SVC\n",
    " #   (X_train, Y_train) = extract_bag_of_words(text_dataset_train)\n",
    "  #  (X_test, Y_test) = extract_bag_of_words(text_dataset_test) # optionally the two calls can be replaced by a single extract_bag_of_words_train_test() function\n",
    "    (X_train,Y_train,X_test,Y_test) = extract_bag_of_words_train_test(text_dataset_train, text_dataset_test)    \n",
    "    sc_text = SVC(kernel = 'rbf', decision_function_shape='ovr', C = 10, gamma = 1.0)\n",
    "    sc_text.fit(X_train, Y_train)\n",
    "    y_pred = sc_text.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, y_pred)\n",
    "    return acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
